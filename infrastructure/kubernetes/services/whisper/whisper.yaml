# Whisper Speech-to-Text Service
# Version: Faster-Whisper (optimized)
# Purpose: Multilingual STT with native Spanish support
# Model: large-v3 (best accuracy for Spanish)
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: whisper-config
  namespace: homelab
  labels:
    app: whisper
data:
  # Model configuration
  ASR_MODEL: "large-v3"  # Best model for Spanish
  ASR_ENGINE: "faster_whisper"  # Optimized inference (4x faster)

  # Language settings
  ASR_LANGUAGE: "es"  # Default to Spanish
  ASR_DETECT_LANGUAGE: "true"  # Auto-detect if not Spanish

  # Performance tuning
  ASR_DEVICE: "cpu"  # Use CPU (GPU on compute node is for LLM)
  ASR_COMPUTE_TYPE: "int8"  # Quantized for faster CPU inference
  ASR_BEAM_SIZE: "5"  # Good balance of speed/accuracy

  # API configuration
  ASR_MODEL_PATH: "/app/models"
  PORT: "9000"

---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: whisper-models-pvc
  namespace: homelab
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 5Gi  # Model storage (~3GB for large-v3)
  storageClassName: local-path

---
apiVersion: v1
kind: Service
metadata:
  name: whisper
  namespace: homelab
  labels:
    app: whisper
spec:
  type: ClusterIP
  ports:
    - port: 9000
      targetPort: 9000
      protocol: TCP
      name: http
  selector:
    app: whisper

---
apiVersion: v1
kind: Service
metadata:
  name: whisper-nodeport
  namespace: homelab
  labels:
    app: whisper
spec:
  type: NodePort
  ports:
    - port: 9000
      targetPort: 9000
      protocol: TCP
      name: http
      nodePort: 30900  # External access for testing
  selector:
    app: whisper

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: whisper
  namespace: homelab
  labels:
    app: whisper
spec:
  replicas: 1  # CPU-intensive, one instance sufficient
  selector:
    matchLabels:
      app: whisper
  template:
    metadata:
      labels:
        app: whisper
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "9000"
        prometheus.io/path: "/health"
    spec:
      nodeSelector:
        workload-type: compute-intensive  # Run on compute node (pesubuntu)
      initContainers:
      - name: model-downloader
        image: python:3.10-slim
        command: ["/bin/sh", "-c"]
        args:
          - |
            echo "Starting model download..."
            pip install -q huggingface-hub && \
            python3 -c "
            from huggingface_hub import snapshot_download
            import os
            model_path = '/app/models/large-v3'
            if os.path.exists(model_path) and os.listdir(model_path):
                print('Model already exists, skipping download')
            else:
                print('Downloading model from HuggingFace...')
                snapshot_download(
                    repo_id='Systran/faster-whisper-large-v3',
                    local_dir=model_path,
                    local_dir_use_symlinks=False
                )
                print('Model downloaded successfully')
            "
        volumeMounts:
        - name: models
          mountPath: /app/models
        resources:
          requests:
            cpu: 100m
            memory: 512Mi
          limits:
            cpu: 500m
            memory: 1Gi
      containers:
      - name: whisper
        # Using faster-whisper optimized container
        image: onerahmet/openai-whisper-asr-webservice:latest-gpu
        imagePullPolicy: IfNotPresent
        ports:
        - containerPort: 9000
          name: http
          protocol: TCP
        env:
        - name: ASR_MODEL_PATH
          value: "/app/models/large-v3"
        envFrom:
        - configMapRef:
            name: whisper-config
        volumeMounts:
        - name: models
          mountPath: /app/models
        resources:
          requests:
            cpu: 250m
            memory: 4Gi
          limits:
            cpu: 2000m  # Allow bursting for transcription
            memory: 8Gi  # Increased for large-v3 model with runtime overhead
        # Health probes disabled - this image doesn't have a /health endpoint
        # Application starts successfully on port 9000
        # livenessProbe:
        #   tcpSocket:
        #     port: 9000
        #   initialDelaySeconds: 60
        #   periodSeconds: 30
        # readinessProbe:
        #   tcpSocket:
        #     port: 9000
        #   initialDelaySeconds: 30
        #   periodSeconds: 10
      volumes:
      - name: models
        persistentVolumeClaim:
          claimName: whisper-models-pvc
      restartPolicy: Always

---
# Horizontal Pod Autoscaler (optional - for high load)
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: whisper-hpa
  namespace: homelab
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: whisper
  minReplicas: 1
  maxReplicas: 3
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 80
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
