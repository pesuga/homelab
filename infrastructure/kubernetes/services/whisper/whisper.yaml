# Whisper Speech-to-Text Service
# Version: Faster-Whisper (optimized)
# Purpose: Multilingual STT with native Spanish support
# Model: large-v3 (best accuracy for Spanish)
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: whisper-config
  namespace: homelab
  labels:
    app: whisper
data:
  # Model configuration
  ASR_MODEL: "large-v3"  # Best model for Spanish
  ASR_ENGINE: "faster_whisper"  # Optimized inference (4x faster)

  # Language settings
  ASR_LANGUAGE: "es"  # Default to Spanish
  ASR_DETECT_LANGUAGE: "true"  # Auto-detect if not Spanish

  # Performance tuning
  ASR_DEVICE: "cpu"  # Use CPU (GPU on compute node is for LLM)
  ASR_COMPUTE_TYPE: "int8"  # Quantized for faster CPU inference
  ASR_BEAM_SIZE: "5"  # Good balance of speed/accuracy

  # API configuration
  ASR_MODEL_PATH: "/app/models"
  PORT: "9000"

---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: whisper-models-pvc
  namespace: homelab
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 5Gi  # Model storage (~3GB for large-v3)
  storageClassName: local-path

---
apiVersion: v1
kind: Service
metadata:
  name: whisper
  namespace: homelab
  labels:
    app: whisper
spec:
  type: ClusterIP
  ports:
    - port: 9000
      targetPort: 9000
      protocol: TCP
      name: http
  selector:
    app: whisper

---
apiVersion: v1
kind: Service
metadata:
  name: whisper-nodeport
  namespace: homelab
  labels:
    app: whisper
spec:
  type: NodePort
  ports:
    - port: 9000
      targetPort: 9000
      protocol: TCP
      name: http
      nodePort: 30900  # External access for testing
  selector:
    app: whisper

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: whisper
  namespace: homelab
  labels:
    app: whisper
spec:
  replicas: 1  # CPU-intensive, one instance sufficient
  selector:
    matchLabels:
      app: whisper
  template:
    metadata:
      labels:
        app: whisper
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "9000"
        prometheus.io/path: "/health"
    spec:
      nodeSelector:
        workload-type: compute-intensive  # Run on compute node (pesubuntu)
      containers:
      - name: whisper
        # Using faster-whisper optimized container
        image: onerahmet/openai-whisper-asr-webservice:latest-gpu
        imagePullPolicy: IfNotPresent
        ports:
        - containerPort: 9000
          name: http
          protocol: TCP
        envFrom:
        - configMapRef:
            name: whisper-config
        volumeMounts:
        - name: models
          mountPath: /app/models
        resources:
          requests:
            cpu: 250m
            memory: 1Gi
          limits:
            cpu: 2000m  # Allow bursting for transcription
            memory: 2Gi
        livenessProbe:
          httpGet:
            path: /health
            port: 9000
          initialDelaySeconds: 60  # Model loading takes time
          periodSeconds: 30
          timeoutSeconds: 10
          failureThreshold: 3
        readinessProbe:
          httpGet:
            path: /health
            port: 9000
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 3
      volumes:
      - name: models
        persistentVolumeClaim:
          claimName: whisper-models-pvc
      restartPolicy: Always

---
# Horizontal Pod Autoscaler (optional - for high load)
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: whisper-hpa
  namespace: homelab
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: whisper
  minReplicas: 1
  maxReplicas: 3
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 80
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
