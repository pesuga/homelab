apiVersion: apps/v1
kind: Deployment
metadata:
  name: llamacpp-mistral
  namespace: llamacpp
  labels:
    app.kubernetes.io/name: llamacpp
    app.kubernetes.io/component: deployment
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: llamacpp
      app.kubernetes.io/component: deployment
  template:
    metadata:
      labels:
        app.kubernetes.io/name: llamacpp
        app.kubernetes.io/component: deployment
    spec:
      hostNetwork: false  # Use pod network
      hostPID: true      # Use host PID for GPU device access
      containers:
      - name: llamacpp-server
        image: ubuntu:24.04
        imagePullPolicy: IfNotPresent
        command: ["/bin/bash", "-c"]
        args:
          - |
            echo "Starting llama.cpp server with Mistral-7B-OpenOrca..."
            echo "Configuration:"
            echo "  Model: ${MODEL_PATH}"
            echo "  Port: ${PORT}"
            echo "  Parallel slots: ${N_PARALLEL}"
            echo "  Context size: ${CTX_SIZE}"
            echo "  GPU layers: ${N_GPU_LAYERS}"

            # Set GPU environment
            export HSA_OVERRIDE_GFX_VERSION=11.0.0
            export ROCR_VISIBLE_DEVICES=0
            export ROCM_PATH=/opt/rocm-6.4.1

            # Use pre-built llama.cpp from host with GPU support
            exec /host/llama.cpp/bin/llama-server \
              -m ${MODEL_PATH} \
              --host ${HOST} \
              --port ${PORT} \
              -np ${N_PARALLEL} \
              --n-gpu-layers ${N_GPU_LAYERS} \
              --ctx-size ${CTX_SIZE} \
              --batch-size ${BATCH_SIZE} \
              --ubatch-size ${U_BATCH_SIZE} \
              --threads ${N_THREADS} \
              --metrics
        envFrom:
        - configMapRef:
            name: llamacpp-config
        ports:
        - containerPort: 8080
          name: http
          protocol: TCP
        resources:
          requests:
            memory: "8Gi"
            cpu: "2000m"
          limits:
            memory: "16Gi"
            cpu: "4000m"
        livenessProbe:
          httpGet:
            path: /health
            port: http
          initialDelaySeconds: 60
          periodSeconds: 30
          timeoutSeconds: 10
        readinessProbe:
          httpGet:
            path: /health
            port: http
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
        lifecycle:
          preStop:
            exec:
              command: ["/bin/sh", "-c", "pkill llama-server || true"]
        volumeMounts:
        - name: host-models
          mountPath: /models
        - name: host-llamacpp
          mountPath: /host/llama.cpp
          readOnly: true
        - name: host-dev
          mountPath: /dev
        - name: host-rocm
          mountPath: /opt/rocm-6.4.1
          readOnly: true
      volumes:
      - name: host-models
        hostPath:
          path: /home/pesu/models
          type: Directory
      - name: host-llamacpp
        hostPath:
          path: /home/pesu/llama.cpp
          type: Directory
      - name: host-dev
        hostPath:
          path: /dev
          type: Directory
      - name: host-rocm
        hostPath:
          path: /opt/rocm-6.4.1
          type: DirectoryOrCreate
      nodeSelector:
        kubernetes.io/hostname: pesubuntu
