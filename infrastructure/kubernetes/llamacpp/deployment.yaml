apiVersion: apps/v1
kind: Deployment
metadata:
  name: llamacpp-kimi-vl
  namespace: llamacpp
  labels:
    app.kubernetes.io/name: llamacpp
    app.kubernetes.io/component: deployment
    app.kubernetes.io/instance: kimi-vl
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: llamacpp
      app.kubernetes.io/component: deployment
      app.kubernetes.io/instance: kimi-vl
  template:
    metadata:
      labels:
        app.kubernetes.io/name: llamacpp
        app.kubernetes.io/component: deployment
        app.kubernetes.io/instance: kimi-vl
    spec:
      hostNetwork: true  # Use host network for GPU access
      hostPID: true      # Use host PID for GPU device access
      containers:
      - name: llamacpp-server
        image: ubuntu:24.04
        imagePullPolicy: IfNotPresent
        command: ["/bin/bash", "-c"]
        args:
          - |
            echo "Setting up environment for GPU access..."
            export HSA_OVERRIDE_GFX_VERSION=11.0.0
            export ROCR_VISIBLE_DEVICES=0
            export ROCM_PATH=/opt/rocm-6.4.1

            echo "Installing runtime dependencies only..."
            apt-get update -qq
            apt-get install -y --no-install-recommends \
              curl wget jq \
              libgl1 libglvnd0 libegl1 libgles2 \
              libvulkan1 vulkan-tools \
              libcurl4-openssl-dev \
              python3 python3-pip

            echo "Copying pre-built llama.cpp..."
            mkdir -p /tmp/llamacpp/bin
            cp -r /home/pesu/llama.cpp/bin/* /tmp/llamacpp/bin/ 2>/dev/null || echo "Will build from source"

            if [ ! -f /tmp/llamacpp/bin/llama-server ]; then
              echo "Building llama.cpp..."
              apt-get install -y --no-install-recommends \
                build-essential cmake git \
                libgl-dev libglvnd-dev libgl1-mesa-dev \
                libegl1-mesa-dev libgles2-mesa-dev \
                libvulkan-dev vulkan-tools shaderc \
                libshaderc-dev glslc \
                python3 python3-pip
              git clone https://github.com/ggml-org/llama.cpp.git /tmp/llamacpp-source
              cd /tmp/llamacpp-source
              cmake -B build -DGGML_VULKAN=ON -DCMAKE_BUILD_TYPE=Release
              cmake --build build --config Release -j$(nproc)
              cp -r build/* /tmp/llamacpp/
            fi

            echo "Starting llama.cpp server with Kimi-VL model..."
            exec /tmp/llamacpp/bin/llama-server \
              -m ${MODEL_PATH} \
              --mmproj ${MMPROJ_PATH} \
              --host ${HOST} \
              --port ${PORT} \
              -np ${N_PARALLEL} \
              --n-gpu-layers ${N_GPU_LAYERS} \
              --ctx-size ${CTX_SIZE} \
              --batch-size ${BATCH_SIZE} \
              --ubatch-size ${U_BATCH_SIZE} \
              --threads ${N_THREADS} \
              --metrics \
              --log-disable
        envFrom:
        - configMapRef:
            name: llamacpp-config
        ports:
        - containerPort: 8080
          name: http
          protocol: TCP
        volumeMounts:
        - name: models-storage
          mountPath: /models
        resources:
          requests:
            memory: "8Gi"
            cpu: "2000m"
          limits:
            memory: "16Gi"
            cpu: "4000m"
        livenessProbe:
          httpGet:
            path: /health
            port: http
          initialDelaySeconds: 120
          periodSeconds: 30
          timeoutSeconds: 10
        readinessProbe:
          httpGet:
            path: /health
            port: http
          initialDelaySeconds: 60
          periodSeconds: 10
          timeoutSeconds: 5
        lifecycle:
          preStop:
            exec:
              command: ["/bin/sh", "-c", "pkill llama-server || true"]
        volumeMounts:
        - name: models-storage
          mountPath: /models
        - name: host-llamacpp
          mountPath: /home/pesu/llama.cpp
          readOnly: true
        - name: host-models
          mountPath: /home/pesu/models
          readOnly: true
      volumes:
      - name: models-storage
        persistentVolumeClaim:
          claimName: llamacpp-models-pvc
      - name: host-llamacpp
        hostPath:
          path: /home/pesu/llama.cpp
          type: Directory
      - name: host-models
        hostPath:
          path: /home/pesu/models
          type: Directory
      nodeSelector:
        kubernetes.io/hostname: pesubuntu  # Deploy to compute node with GPU