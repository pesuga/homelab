apiVersion: apps/v1
kind: Deployment
metadata:
  name: llamacpp-mistral
  namespace: llamacpp
  labels:
    app.kubernetes.io/name: llamacpp
    app.kubernetes.io/component: deployment
    app.kubernetes.io/instance: mistral
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: llamacpp
      app.kubernetes.io/component: deployment
      app.kubernetes.io/instance: mistral
  template:
    metadata:
      labels:
        app.kubernetes.io/name: llamacpp
        app.kubernetes.io/component: deployment
        app.kubernetes.io/instance: mistral
    spec:
      hostNetwork: false
      hostPID: true
      containers:
      - name: llamacpp-server
        image: ubuntu:24.04
        imagePullPolicy: IfNotPresent
        command: ["/bin/bash", "-c"]
        args:
          - |
            echo "Setting up environment for GPU access..."
            export HSA_OVERRIDE_GFX_VERSION=11.0.0
            export ROCR_VISIBLE_DEVICES=0
            export ROCM_PATH=/opt/rocm-6.4.1

            echo "Installing runtime dependencies only..."
            apt-get update -qq
            apt-get install -y --no-install-recommends \
              curl wget jq \
              libgl1 libglvnd0 libegl1 libgles2 \
              libvulkan1 vulkan-tools \
              libcurl4-openssl-dev \
              libgomp1 \
              python3 python3-pip

            echo "Building llama.cpp from source..."
            mkdir -p /tmp/llamacpp/bin

            if true; then
              echo "Setting up build environment..."
              apt-get install -y --no-install-recommends \
                build-essential cmake git \
                libgl-dev libglvnd-dev libgl1-mesa-dev \
                libegl1-mesa-dev libgles2-mesa-dev \
                libvulkan-dev vulkan-tools \
                libshaderc-dev glslang-tools \
                python3 python3-pip
              git clone https://github.com/ggml-org/llama.cpp.git /tmp/llamacpp-source
              cd /tmp/llamacpp-source
              cmake -B build -DCMAKE_BUILD_TYPE=Release
              cmake --build build --config Release -j$(nproc)
              cp -r build/* /tmp/llamacpp/
            fi

            echo "Starting llama.cpp server with Mistral-7B-OpenOrca model..."
            exec /tmp/llamacpp/bin/llama-server \
              -m ${MODEL_PATH} \
              --host ${HOST} \
              --port ${PORT} \
              -np ${N_PARALLEL} \
              --n-gpu-layers ${N_GPU_LAYERS} \
              --ctx-size ${CTX_SIZE} \
              --batch-size ${BATCH_SIZE} \
              --ubatch-size ${U_BATCH_SIZE} \
              --threads ${N_THREADS} \
              --metrics
        envFrom:
        - configMapRef:
            name: llamacpp-mistral-config
        ports:
        - containerPort: 8081
          name: http
          protocol: TCP
        resources:
          requests:
            memory: "6Gi"
            cpu: "2000m"
          limits:
            memory: "12Gi"
            cpu: "4000m"
        livenessProbe:
          httpGet:
            path: /health
            port: http
          initialDelaySeconds: 120
          periodSeconds: 30
          timeoutSeconds: 10
        readinessProbe:
          httpGet:
            path: /health
            port: http
          initialDelaySeconds: 60
          periodSeconds: 10
          timeoutSeconds: 5
        lifecycle:
          preStop:
            exec:
              command: ["/bin/sh", "-c", "pkill llama-server || true"]
        volumeMounts:
        - name: host-models
          mountPath: /models
      volumes:
      - name: host-models
        hostPath:
          path: /home/pesu/models
          type: Directory
      nodeSelector:
        kubernetes.io/hostname: pesubuntu
