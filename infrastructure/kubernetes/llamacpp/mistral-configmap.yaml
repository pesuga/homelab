apiVersion: v1
kind: ConfigMap
metadata:
  name: llamacpp-config
  namespace: llamacpp
  labels:
    app.kubernetes.io/name: llamacpp
    app.kubernetes.io/component: config
data:
  HOST: "0.0.0.0"
  PORT: "8080"
  N_PARALLEL: "2"           # Support 2 concurrent inferences
  N_GPU_LAYERS: "35"        # Mistral-7B has 32 layers, load all
  CTX_SIZE: "16384"         # 16K context window
  BATCH_SIZE: "512"
  U_BATCH_SIZE: "128"
  N_THREADS: "8"
  LOG_DISABLE: "0"
  MODEL_PATH: "/models/mistral-7b-openorca.Q5_K_M.gguf"
