# Memory-Enabled RAG Agent: Architecture Analysis & Recommendations

**Date**: 2025-10-30
**Status**: Analysis Complete - Recommendations Ready

## Your Requirements

1. **Chat Interface** for Flowise/N8n agent
2. **mem0 Integration** for persistent memory (remembers you over time)
3. **RAG Capability** with Qdrant vector store
4. **Spanish STT** (Speech-to-Text)

## Current Stack Analysis

### âœ… What You Already Have

| Component | Status | Location | Notes |
|-----------|--------|----------|-------|
| **mem0** | âœ… Running (2 replicas) | http://mem0.homelab.svc.cluster.local:8080 | Already configured with Qdrant + Ollama |
| **Qdrant** | âœ… Running | http://qdrant.homelab.svc.cluster.local:6333 | 20Gi storage, ready for RAG |
| **Flowise** | âœ… Running | http://100.81.76.55:30850 | Has mem0 integration in v2.2.8+ |
| **N8n** | âœ… Running | http://100.81.76.55:30678 | Can integrate with mem0 API |
| **Open WebUI** | âœ… Running | http://100.81.76.55:30080 | Chat interface, but limited mem0 integration |
| **Ollama** | âœ… Running | http://100.72.98.106:11434 | 82 tok/s GPU-accelerated |
| **PostgreSQL** | âœ… Running | postgres.homelab.svc.cluster.local | Shared database |

### ğŸ”´ What You're Missing

| Need | Gap | Recommendation |
|------|-----|----------------|
| **Spanish STT** | None deployed | Add Whisper (supports Spanish natively) |
| **Native mem0 Chat UI** | Open WebUI has limited support | Deploy **LobeChat** (native mem0 + STT support) |
| **TTS for responses** | No voice output | Whisper + Piper TTS |

## Architecture Comparison

### Option 1: Open WebUI + Flowise + mem0 (Current Plan)
```
User â†’ Open WebUI â†’ Pipeline â†’ Flowise â†’ mem0 API â†’ Memory
                    â†“
                  Ollama â†’ Response
```

**Pros**:
- âœ… Minimal new deployments
- âœ… Already have all components

**Cons**:
- âŒ Complex pipeline setup
- âŒ No native mem0 integration in Open WebUI
- âŒ No built-in STT support
- âŒ Requires custom pipeline code
- âŒ Three-hop latency (WebUI â†’ Flowise â†’ mem0)

**Verdict**: âš ï¸ **Overcomplicated** - Too many hops, custom code needed

---

### Option 2: LobeChat + mem0 + Qdrant (RECOMMENDED)
```
User (voice/text) â†’ LobeChat â†’ mem0 API â†’ Memory Store
                    â†“               â†“
                  Ollama        Qdrant (RAG)
                    â†“
                  Response (voice/text)
```

**Pros**:
- âœ… **Native mem0 integration** (built-in, no custom code)
- âœ… **Native Spanish STT/TTS** (Whisper + Piper built-in)
- âœ… **Modern UI** with voice interface
- âœ… **Direct integration** - one hop to mem0
- âœ… **RAG support** with Qdrant
- âœ… **Self-hosted** and open source
- âœ… **One-click deployment** on K8s
- âœ… **Mobile-friendly** (important for voice)

**Cons**:
- âš ï¸ Need to deploy LobeChat (one new service)
- âš ï¸ Need to deploy Whisper (for STT)

**Verdict**: âœ… **BEST CHOICE** - Purpose-built for your exact use case

---

### Option 3: N8n Direct + mem0 API
```
User â†’ N8n Webhook â†’ mem0 API â†’ Memory
         â†“
       Ollama â†’ Response
```

**Pros**:
- âœ… Full workflow control
- âœ… Good for automation

**Cons**:
- âŒ N8n not designed as chat interface
- âŒ No voice support
- âŒ Poor UX for conversation
- âŒ Better for background automation

**Verdict**: âŒ **Wrong Tool** - N8n is for workflows, not chat

---

### Option 4: Flowise Native (Simpler Alternative)
```
User â†’ Flowise UI â†’ mem0 Node â†’ Memory
                  â†“           â†“
                Ollama      Qdrant
```

**Pros**:
- âœ… Flowise has built-in mem0 node (v2.2.8+)
- âœ… Visual workflow builder
- âœ… RAG chain building
- âœ… Already deployed

**Cons**:
- âš ï¸ Flowise UI is for building, not daily chat
- âŒ No STT/TTS support
- âŒ Not mobile-friendly
- âŒ Clunky for conversation

**Verdict**: âš ï¸ **Good for Development** - Use for building agents, not chatting

---

## Detailed Recommendation: LobeChat Stack

### Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   User Interface                     â”‚
â”‚  LobeChat (http://100.81.76.55:30900)               â”‚
â”‚  â€¢ Text + Voice input (Spanish STT via Whisper)     â”‚
â”‚  â€¢ Voice output (TTS via Piper)                      â”‚
â”‚  â€¢ Mobile-friendly PWA                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                   â”‚
        â–¼                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   mem0 API    â”‚   â”‚  Ollama LLM  â”‚
â”‚  (Memory)     â”‚   â”‚  82 tok/s    â”‚
â”‚               â”‚   â”‚  GPU accel   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Qdrant Vector DB â”‚
â”‚ (RAG + Memory)   â”‚
â”‚ â€¢ User memories  â”‚
â”‚ â€¢ Document store â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Component Breakdown

**1. LobeChat** (New - Primary Chat UI)
- Modern React-based chat interface
- **Native mem0 support** (no custom code)
- **Multilingual STT/TTS** including Spanish
- Knowledge base (file upload â†’ RAG)
- Plugin marketplace
- Mobile PWA support
- Self-hosted Docker/K8s deployment

**2. mem0** (Already Running)
- API endpoint: `http://mem0.homelab.svc.cluster.local:8080`
- Already configured with:
  - Qdrant backend
  - Ollama LLM (mistral:7b)
  - Embeddings (nomic-embed-text)
- LobeChat connects directly via API

**3. Qdrant** (Already Running)
- Vector DB for:
  - mem0 memory storage
  - RAG document embeddings
  - Semantic search
- Already has 20Gi storage

**4. Whisper** (New - STT Engine)
- Open-source multilingual STT
- **Native Spanish support** (trained on 680k hours)
- Can run locally on CPU or GPU
- LobeChat connects via API

**5. Ollama** (Already Running)
- LLM backend: gpt-oss:20b at 82 tok/s
- Embeddings: nomic-embed-text
- GPU-accelerated on compute node

### Spanish STT Options

#### Option A: Whisper (Recommended)
- **Model**: openai/whisper-large-v3
- **Spanish Support**: Excellent (native multilingual)
- **Deployment**: Docker container
- **Resource**: CPU (500-1000m) or GPU offload
- **Latency**: ~1-2s for 10s audio
- **Accuracy**: State-of-the-art for Spanish

```yaml
# Whisper deployment (simplified)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: whisper
  namespace: homelab
spec:
  containers:
  - name: whisper
    image: onerahmet/openai-whisper-asr-webservice:latest
    env:
    - name: ASR_MODEL
      value: "large-v3"
    - name: ASR_ENGINE
      value: "faster_whisper"
```

#### Option B: Faster-Whisper (Optimized)
- **Speed**: 4x faster than Whisper
- **Accuracy**: Same model, optimized inference
- **Resource**: Lower CPU/memory usage
- **Better for**: Real-time transcription

### Deployment Plan

#### Step 1: Deploy Whisper STT
```bash
# Create Whisper deployment
kubectl apply -f infrastructure/kubernetes/services/whisper/whisper.yaml

# Service endpoint: whisper.homelab.svc.cluster.local:9000
```

#### Step 2: Deploy LobeChat
```bash
# Create LobeChat deployment with config:
# - Ollama endpoint: http://100.72.98.106:11434
# - mem0 endpoint: http://mem0.homelab.svc.cluster.local:8080
# - Whisper endpoint: http://whisper.homelab.svc.cluster.local:9000

kubectl apply -f infrastructure/kubernetes/services/lobechat/lobechat.yaml

# Access: http://100.81.76.55:30900
```

#### Step 3: Configure LobeChat
```javascript
// LobeChat config (environment variables)
{
  "OLLAMA_BASE_URL": "http://100.72.98.106:11434",
  "MEM0_API_URL": "http://mem0.homelab.svc.cluster.local:8080",
  "WHISPER_API_URL": "http://whisper.homelab.svc.cluster.local:9000",
  "QDRANT_URL": "http://qdrant.homelab.svc.cluster.local:6333",
  "DEFAULT_LANGUAGE": "es"  // Spanish
}
```

#### Step 4: Configure mem0 Integration
```javascript
// LobeChat mem0 plugin config
{
  "memory": {
    "provider": "mem0",
    "config": {
      "apiUrl": "http://mem0.homelab.svc.cluster.local:8080",
      "userId": "user_default",  // Or dynamic per user
      "enableMemory": true,
      "memoryRetention": "permanent"
    }
  }
}
```

### How It Works (User Flow)

1. **User speaks in Spanish** â†’ Whisper transcribes â†’ Text
2. **LobeChat sends to mem0** â†’ Extracts user facts â†’ Stores in Qdrant
3. **mem0 recalls relevant memories** â†’ Adds context to prompt
4. **Ollama generates response** â†’ 82 tok/s GPU-accelerated
5. **LobeChat reads response** â†’ TTS (optional) â†’ User hears

**Example Conversation**:
```
User (Spanish): "Hola, mi nombre es Pedro y me gusta el ciclismo"
â†“ Whisper STT
LobeChat â†’ mem0 API
â†“ mem0 stores: {"user": "Pedro", "interests": ["cycling"]}
â†“ Ollama (with context)
Response: "Hola Pedro! Es un placer conocerte. El ciclismo es genial..."

[Next day]
User: "QuÃ© rutas de ciclismo me recomiendas?"
â†“ mem0 recalls: User is Pedro, likes cycling
â†“ Ollama (with memory context)
Response: "Hola Pedro! Como te gusta el ciclismo, te recomiendo..."
```

### Comparison: LobeChat vs Open WebUI

| Feature | LobeChat | Open WebUI | Winner |
|---------|----------|------------|--------|
| **mem0 Integration** | Native, built-in | Manual pipeline | âœ… LobeChat |
| **Spanish STT** | Native support | Requires custom setup | âœ… LobeChat |
| **Voice UI** | Built-in, mobile-ready | Limited | âœ… LobeChat |
| **RAG** | Knowledge base + Qdrant | Plugin system | ğŸŸ° Tie |
| **Deployment** | Simple Docker/K8s | Simple Docker/K8s | ğŸŸ° Tie |
| **Customization** | Plugin marketplace | Pipeline system | ğŸŸ° Tie |
| **Mobile** | PWA, excellent | Good | âœ… LobeChat |
| **Documentation** | Excellent | Good | âœ… LobeChat |
| **Active Dev** | Very active | Active | âœ… LobeChat |

**Verdict**: âœ… LobeChat is purpose-built for your exact requirements

### Resource Requirements

**New Services**:
```yaml
Whisper:
  CPU: 500m-1000m
  Memory: 2Gi
  Storage: 2Gi (model)

LobeChat:
  CPU: 250m
  Memory: 512Mi
  Storage: 1Gi
```

**Total New Resources**: ~3.5Gi RAM, ~1 CPU core

**Service Node Capacity**:
- Current: 8GB RAM, 98GB disk
- After: ~6.5GB free RAM remaining âœ… Acceptable

## Alternative: Keep It Simple with Flowise

If you want **minimal new deployments**, you can use Flowise's native mem0 support:

### Flowise mem0 Setup (Already Possible)

1. **In Flowise** (http://100.81.76.55:30850):
   - Create new chatflow
   - Add "Chat Model" â†’ Ollama (gpt-oss:20b)
   - Add "Memory" â†’ **mem0 Memory** node
   - Configure mem0:
     - API URL: `http://mem0.homelab.svc.cluster.local:8080`
     - User ID: `pedro` (or dynamic)
   - Add "Retrieval QA Chain" for RAG
   - Connect to Qdrant vector store
   - Deploy chatflow

2. **Access via**:
   - Flowise built-in chat UI
   - Or N8n webhook â†’ Flowise API
   - Or Custom React app â†’ Flowise API

**Pros**: Zero new deployments, use what you have
**Cons**: No voice, clunky UI, not mobile-friendly

## Final Recommendation

### For Your Use Case: **Deploy LobeChat**

**Why**:
1. âœ… **Native mem0** - No custom code, just works
2. âœ… **Spanish STT** - Whisper built-in support
3. âœ… **Voice-first** - Designed for conversation
4. âœ… **Mobile-ready** - Use from phone with voice
5. âœ… **Modern UX** - Clean, fast, professional
6. âœ… **RAG support** - Qdrant integration
7. âœ… **Minimal overhead** - ~3.5Gi resources

**Keep**:
- Flowise: For building complex agent workflows
- N8n: For automation and background tasks
- LobeChat: For daily conversational interaction

**Architecture**:
```
Daily Chat â†’ LobeChat (mem0 + voice)
Agent Development â†’ Flowise (visual builder)
Automation â†’ N8n (workflows)
All share â†’ Ollama (GPU) + Qdrant (vectors) + mem0 (memory)
```

## Implementation Checklist

### Phase 1: Deploy STT (Week 1)
- [ ] Create Whisper deployment YAML
- [ ] Deploy to homelab namespace
- [ ] Test Spanish transcription
- [ ] Verify resource usage

### Phase 2: Deploy LobeChat (Week 1)
- [ ] Create LobeChat deployment YAML
- [ ] Configure Ollama connection
- [ ] Configure mem0 integration
- [ ] Configure Whisper STT
- [ ] Test chat interface

### Phase 3: Configure Memory (Week 1)
- [ ] Set up user profiles in mem0
- [ ] Test memory persistence
- [ ] Verify Qdrant storage
- [ ] Test Spanish conversation flow

### Phase 4: Add RAG (Week 2)
- [ ] Upload documents to Qdrant via LobeChat
- [ ] Configure knowledge base
- [ ] Test document Q&A
- [ ] Test memory + RAG together

### Phase 5: Polish (Week 2)
- [ ] Configure TTS for voice responses
- [ ] Set up mobile PWA
- [ ] Create monitoring dashboard
- [ ] Document user guide

## Estimated Timeline

- **Whisper Deployment**: 2 hours
- **LobeChat Deployment**: 3 hours
- **Configuration & Testing**: 4 hours
- **Total**: ~1 day of focused work

## Next Steps

1. **Review this analysis** - Confirm LobeChat approach
2. **I'll create deployment YAMLs** - Ready to apply
3. **Deploy and test** - Get it running
4. **Iterate** - Refine based on usage

Would you like me to proceed with creating the LobeChat + Whisper deployment manifests?
