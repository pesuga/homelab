# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Project Overview

This is a **homelab agentic workflow platform** - a self-hosted infrastructure for running AI agents and workflows with local LLM inference. The platform consists of two primary nodes:

- **Compute Node**: Native Ubuntu 25.10 (pesubuntu) - runs Ollama with GPU acceleration (AMD RX 7800 XT) and Whisper
- **Service Node**: Ubuntu Server 24.04 (asuna, 192.168.8.185) running K3s with N8n, PostgreSQL, Redis, Qdrant, and Loki

**Current Status**: Sprint 4 ‚úÖ COMPLETED - Advanced services deployed, ROCm + Ollama functional, Family Assistant Enhancement in progress 

## Architecture Overview

### Two-Node Architecture

**Compute Node (pesubuntu - localhost)**:
- Purpose: LLM inference with GPU acceleration
- Hardware: i5-12400F (6C/12T), 32GB RAM, AMD RX 7800 XT (16GB VRAM, Navi 32)
- Services: Ollama (native + K8s ready), ROCm 6.4.1, Promtail, Tailscale
- OS: Ubuntu 25.10 (Questing Quetzal) - Native installation
- Kernel: 6.17.0-5-generic
- Storage: 937GB available
- Current State: ‚úÖ ROCm 6.4.1 installed, ‚úÖ Ollama 0.12.6 with GPU acceleration, ‚úÖ Models loaded, ‚úÖ K8s manifests ready

**Service Node (asuna - 192.168.8.185)**:
- Purpose: Kubernetes orchestration and workflow automation
- Hardware: i7-4510U, 8GB RAM, 98GB storage
- Services: K3s v1.33.5, N8n, PostgreSQL 16.10, Redis 7.4.6, Qdrant v1.12.5, Mem0, Loki 2.9.3, Promtail, Prometheus, Homelab Dashboard, LobeChat, Docker Registry, Whisper STT, Ollama (GPU-accelerated)
- OS: Ubuntu 24.04.3 LTS
- Tailscale IP: 100.81.76.55
- K3s configured with Tailscale IP in TLS SAN for remote kubectl access
- Database Services:
  - PostgreSQL: postgres.homelab.svc.cluster.local:5432 (User: homelab, DB: homelab, 10Gi storage)
  - Redis: redis.homelab.svc.cluster.local:6379 (ephemeral storage, AOF enabled)
  - Qdrant: qdrant.homelab.svc.cluster.local:6333 (HTTP), :6334 (gRPC), 20Gi storage
- GitOps: Flux CD structure ready for bootstrap 

### Networking

- **IMPORTANT**: Always use Tailscale IPs for inter-node communication
- Local Network: 192.168.8.0/24 (note: some docs reference 192.168.8.0/24, actual is 192.168.8.0/24)
- Tailscale Mesh: 100.64.0.0/10 for secure remote access
- Service Node (asuna): 100.81.76.55
- Compute Node (pesubuntu): 100.72.98.106

## Claude Code Session Management

### Documentation Structure

This project follows a structured documentation approach for Claude Code sessions:

#### Core Documentation Sections
- **`session-state`**: Track what happened during each session (commands, files, outcomes, decisions)
- **`sprint-updates`**: Progress on current sprint objectives andÈáåÁ®ãÁ¢ë (milestones)
- **`architecture-changes`**: Design decisions, new patterns, structural modifications
- **`services-inventory`**: New/modified/deleted services and their current status
- **`knowledge-capture`**: Reusable patterns, code snippets, lessons learned, best practices
- **`blocking-issues`**: Current impediments, blockers, and their resolution status
- **`next-steps`**: Immediate action items and priorities for the next session

#### Documentation Locations
- **Primary**: `docs/SESSION-STATE.md` - Main session tracking
- **Archive**: `claudedocs/archive/` - Historical session documentation (date-stamped)
- **Temporary**: Session-specific docs in project root (to be cleaned up by end-of-session hook)

#### "Where We Left Off" Section
Located at the end of this file, this section provides context continuity between sessions:
- Current working directory and branch
- Active tasks and their completion status
- Tools and services being used
- Continuation instructions for next session

## Common Commands
- When running commands in the compute node, verify where you are, most likely you are already there.
- For any kubectl command you need to run, there is no need to ssh.
- Anytime we make changes to services (add/remove/change url or ip or any details) we need to ask another agent to update the dashboard with the new details.
- After working on any of the apps (dashboard, family assistant or any other) remember to commit and push so CD gets started.

### LLM Infrastructure (Compute Node)

**Health Check and testing scripts**
List of all testing scripts:
- Health checks `./infrastructure/compute-node/scripts/health-check.sh` to verify LLM services status

### Homelab Dashboard
- The dashboard app should always be up to date with the cluster services
- Always use and show the https pesulabs.net urls for services

### Monitoring & Observability

**Access Dashboards** (HTTPS URLs):
- **Homelab Dashboard**: https://dash.pesulabs.net (admin/ChangeMe!2024#Secure) - Unified landing page ‚úÖ Fixed
- **N8n Workflows**: https://n8n.homelab.pesulabs.net (admin/admin123)
- **Prometheus**: https://prometheus.homelab.pesulabs.net
- **LobeChat**: https://chat.homelab.pesulabs.net (AI chat interface with memory)
- **Family Assistant**: ‚ö†Ô∏è Development in progress - Phase 1: Enhanced Dashboard & Monitoring

**Internal Services** (NodePort access for development):
- **Loki**: http://100.81.76.55:30314 (log aggregation API)
- **Qdrant Vector DB**: http://100.81.76.55:30633 (HTTP API), :6334 (gRPC)
- **Mem0 AI Memory**: http://100.81.76.55:30880 (AI memory layer API)
- **Docker Registry**: http://100.81.76.55:30500 (insecure registry)
- **Ollama API (K8s)**: http://100.81.76.55:30277 (Kubernetes deployment, GPU-accelerated)
- **Whisper STT**: http://100.81.76.55:30900 (speech-to-text service)
- **Family Assistant API**: http://100.81.76.55:30080 (when deployment fixed)

### Qdrant Vector Database

Qdrant provides vector similarity search for RAG applications. See `docs/QDRANT-SETUP.md` for deployment and integration details.

**Integration**: Mem0 uses Qdrant for vector storage and Ollama (nomic-embed-text) for embeddings

### Mem0 AI Memory Layer

Mem0 provides AI memory management using Qdrant for storage and Ollama for embeddings. Access via API endpoint at http://100.81.76.55:30820

### Loki Log Aggregation

Loki aggregates logs from all services and nodes. Query logs via:
- **Direct API**: Loki API endpoint at http://100.81.76.55:30314

Common log sources: homelab namespace pods, systemd services (Ollama), system logs

### GitOps with Flux CD (Ready for Bootstrap)

Flux CD enables declarative infrastructure management via Git. See `docs/GITOPS-SETUP.md` for complete bootstrap and configuration instructions.

Required setup:
- Flux CLI installation
- GitHub token with repo access
- Bootstrap to K3s cluster
- Repository structure in `clusters/homelab/`

### Development Workflow

1. Clone repository: `git clone https://github.com/pesuga/homelab.git`
2. Checkout working branch: `git checkout main`
3. Copy and configure environment: `.env-example` ‚Üí `.env`
4. Run setup script: `./scripts/setup.sh` (minimal - needs expansion)

## Key Architecture Patterns

### LLM Request Flow

**Current Request Path**: User/N8n ‚Üí Ollama API (direct to compute node) ‚Üí GPU acceleration via ROCm
- **Local API**: http://100.72.98.106:11434 (compute node, GPU-accelerated)
- **Models Available**: qwen2.5-coder:14b, llama3.1:8b, mistral:7b-instruct-q4_K_M, nomic-embed-text

**Future Request Path** (when K8s deployed): User/N8n ‚Üí Traefik Ingress (HTTPS) ‚Üí Ollama Service (K8s) ‚Üí Ollama Pod ‚Üí LLM Model ‚Üí GPU
- **K8s API**: https://ollama.homelab.pesulabs.net (planned)
- **Architecture**: Direct GPU access with Kubernetes orchestration and HTTPS termination

### Service Deployment Pattern

All services on the service node run as Kubernetes workloads:
- Base namespace: `homelab`
- Storage: Persistent Volume Claims (10Gi PostgreSQL, 5Gi N8n/Grafana, 20Gi Qdrant/Loki, 20Gi Docker Registry)
- Networking: ClusterIP for internal, NodePort for external access
- No Ingress controller yet - direct NodePort access (HTTPS via homelab.pesulabs.net domains planned)

### Data Persistence Strategy

- **PostgreSQL**: Used by N8n and Family Assistant for storage, uses 10Gi PVC
- **Redis**: In-memory cache and job queue with AOF persistence, uses PVC
- **Qdrant**: Vector embeddings storage for Mem0 and RAG applications, 20Gi PVC
- **Loki**: Log aggregation with 7-day retention, 20Gi PVC
- **Docker Registry**: Container image storage, 20Gi PVC
- **Ollama Models**: Stored in persistent volume (10Gi) managed by Kubernetes
- **Prometheus Metrics**: Time-series data with 10Gi retention PVC

## Current Sprint Status

### Sprint 4: Advanced Services - ‚úÖ COMPLETED

**Completed Objectives**:
1. ‚úÖ Deploy PostgreSQL 16.10 with persistent storage
2. ‚úÖ Deploy Redis 7.4.6 with AOF persistence
3. ‚úÖ Deploy Qdrant v1.12.5 vector database (20Gi storage)
4. ‚úÖ Deploy Mem0 AI memory layer with Qdrant integration
5. ‚úÖ Deploy Loki 2.9.3 log aggregation (20Gi storage, 7-day retention)
6. ‚úÖ Configure Promtail log collection on both nodes
7. ‚úÖ Create Flux CD repository structure
8. ‚úÖ Fix service authentication and security issues
9. ‚úÖ Remove deprecated services (Grafana, Flowise, Open WebUI)
10. ‚úÖ Deploy Ollama to Kubernetes with GPU scheduling
11. ‚úÖ Deploy Whisper STT service with memory optimization
12. ‚úÖ Deploy LobeChat AI interface with memory integration

### Sprint 3: LLM Infrastructure - ‚úÖ COMPLETED

**Completed Objectives**:
1. ‚úÖ Install fresh Ubuntu 25.10 on compute node
2. ‚úÖ Verify GPU detection (AMD RX 7800 XT)
3. ‚úÖ Install ROCm 6.4.1 for AMD GPU support
4. ‚úÖ Deploy Ollama 0.12.6 with GPU acceleration
5. ‚úÖ Load production models: qwen2.5-coder:14b, llama3.1:8b, mistral:7b-instruct-q4_K_M, nomic-embed-text
6. ‚úÖ Configure Ollama systemd service for auto-start
7. ‚úÖ Set up Tailscale on compute node
8. ‚úÖ Create Ollama K8s deployment manifests (ready for Traefik HTTPS)
9. ‚úÖ Deploy Ollama to Kubernetes with proper GPU scheduling
10. ‚úÖ Create comprehensive health check scripts

### üéØ Family Assistant Enhancement (Current Priority)

**Project Vision**: Comprehensive Family Assistant platform with enhanced dashboard, system prompts, MCP tools, user management, bilingual support, and mobile access.

#### Phase 1: Enhanced Dashboard & Monitoring (80% Complete) ‚ö†Ô∏è ARCHITECTURE REVIEW COMPLETE
- ‚úÖ **Dashboard API Endpoints**: Comprehensive system health API with WebSocket support
- ‚úÖ **React Frontend Architecture**: Modern TypeScript application (comprehensive UI built - 8 major areas)
- ‚úÖ **Standalone Dashboard**: Beautiful HTML dashboard with cappuccino moka dark theme
- ‚úÖ **MCP Development Tools**: 5 specialized tools installed and configured

**üèóÔ∏è ARCHITECTURAL FINDINGS** (Review: 2025-11-12):
- **Scope Clarification**: Frontend is comprehensive admin panel, not just monitoring
  - 8 Major Areas: Monitoring, Family Management, Memory Browser, Prompts, Parental Controls, Workflows, Settings, Analytics
- **Critical Gaps Identified**:
  - ‚ùå No JWT authentication (CRITICAL security risk)
  - ‚ùå Coupled frontend/backend deployment (scaling bottleneck)
  - ‚ùå No state management (Context API insufficient for scale)
  - ‚ùå Missing: Memory browser, prompt management, analytics

**üìã WEEK 1 PRIORITIES** (Foundation Hardening):
- [ ] Implement JWT authentication + RBAC (Day 1-2)
- [ ] Separate frontend deployment (nginx) + backend (FastAPI)
- [ ] Add Zustand state management (Day 1)
- [ ] Build API client layer with axios interceptors (Day 2)
- [ ] Connect FamilyMembers page to Phase 2 API (Day 3-4)
- [ ] Add rate limiting + error handling (Day 2-4)

**üìã WEEK 2 PRIORITIES** (Admin Features):
- [ ] Memory browser with semantic search UI
- [ ] Conversation history viewer
- [ ] Prompt management interface
- [ ] Analytics dashboard
- [ ] OpenTelemetry observability

**üìÑ Complete Analysis**: See [ARCHITECTURE_REVIEW_AND_ROADMAP.md](ARCHITECTURE_REVIEW_AND_ROADMAP.md)

#### Phase 2: System Prompts & Memory ‚úÖ COMPLETE (Deployed 2025-11-12)
- ‚úÖ Hierarchical system prompts (5 layers: base ‚Üí safety ‚Üí role ‚Üí language ‚Üí dynamic)
- ‚úÖ Role-based personality adaptation (parent, teenager, child, grandparent)
- ‚úÖ 5-layer memory architecture fully operational (Redis, Mem0, PostgreSQL, Qdrant, Archive)
- ‚úÖ Database schema with pgvector extension for embeddings
- ‚úÖ 11 Phase 2 API endpoints deployed and verified
- ‚úÖ Production image: family-assistant:phase2 (1.09GB)
- See [PHASE2_PRODUCTION_DEPLOYMENT.md](PHASE2_PRODUCTION_DEPLOYMENT.md) for details

#### Phase 3: MCP Integration & User Management (Planned)
- MCP tool connections with RBAC for family members
- User management with parental controls and privacy protection
- Custom workflows with natural language triggers (Spanish/English)
- Full Spanish language support with cultural context

**Infrastructure Next Steps**:
- **Option A**: Deploy Traefik HTTPS ingress for all services
- **Option B**: Start Sprint 5 (Networking & Security)
- **Option C**: Resolve Flux CD network connectivity issues

## Important Files & Locations

### Documentation
- `README.md` - Main project overview and status
- `docs/SESSION-STATE.md` - Current session state and progress tracking (updated daily)
- `docs/LLM-SETUP.md` - Complete AMD GPU + K8s Ollama deployment guide
- `docs/GITOPS-SETUP.md` - Flux CD GitOps setup and multi-repo management
- `docs/QDRANT-SETUP.md` - Qdrant vector database deployment and integration
- `services/family-assistant/` - Family Assistant service with dashboard API
  - `dashboard-standalone.html` - Beautiful cappuccino moka themed dashboard
  - `mcp-tools/` - 5 specialized development tools for homelab workflow enhancement
  - `TELEGRAM_SETUP_GUIDE.md` - Bot configuration and multimodal features
  - `INSTALL.md` - Complete installation and configuration guide

### Configuration
- `.env-example` - Environment variable template (comprehensive)
- `infrastructure/kubernetes/ollama/` - Ollama K8s deployment manifests
- `infrastructure/kubernetes/base/namespace.yaml` - K8s namespace definition

### Service Configurations
- `infrastructure/compute-node/` - Ollama and LiteLLM setup docs
- `infrastructure/kubernetes/` - K8s manifests
  - `base/` - Namespace and core configs
  - `databases/` - PostgreSQL, Redis, Qdrant deployments
  - `monitoring/` - Prometheus, Grafana
  - `monitoring/dashboards/` - Grafana dashboard JSONs and provisioning
  - `flux/` - Flux CD GitOps resources (planned)
- `infrastructure/kubernetes/ollama/` - Ollama K8s manifests
- `services/family-assistant/` - Family Assistant service with FastAPI backend
  - `api/` - FastAPI application with dashboard endpoints and WebSocket support
  - `frontend/` - React TypeScript application structure (components, pages, hooks)
  - `mcp-tools/` - MCP development tools for enhanced workflow
  - `.mcp.json` - Claude Code MCP server configuration

## Development Context

### Built With Claude Code + MCP Tools
This project is developed using AI-assisted pair programming with Claude Code and specialized MCP tools. Key practices:

**Development Workflow Enhancement**:
- **MCP Tools**: 5 specialized tools for homelab development workflow
  - Kubernetes Manager: Advanced cluster management and troubleshooting
  - Frontend Tester: Playwright-based automated UI testing
  - Git Workflow: Intelligent git operations and workflow automation
  - Infrastructure Detective: Network diagnostics and performance analysis
  - Documentation Sync: Automatic documentation synchronization
- **Project-Specific Configuration**: `.mcp.json` with environment variables for each tool
- **Comprehensive Documentation**: Session state tracking in `docs/SESSION-STATE.md`
- **Sprint-Based Development**: Structured roadmap with clear objectives
- **Infrastructure-as-Code**: All configuration version controlled

**Current Development Focus**:
- Family Assistant Enhancement with modern React dashboard
- Real-time system monitoring with WebSocket connections
- MCP integration for enhanced development workflow
- Bilingual support (Spanish/English) with cultural context

### Design Principles
1. **Local-First**: All compute and data stays on-premises
2. **Observable**: Everything logged and monitored via Prometheus/Grafana
3. **Modular**: Services are independent and composable
4. **Open Source**: Built on OSS, no proprietary lock-in
5. **Automated**: Manual work is scripted away

### Technology Choices

**Ollama** (LLM Inference):
- Simple model management with REST API
- AMD GPU support via ROCm
- Active development and community
- Native HTTP API (no proxy needed)

**K3s** (Kubernetes):
- Lightweight K8s perfect for single-node homelab
- Full K8s compatibility
- Easy installation and maintenance

**N8n** (Workflow Automation):
- Low-code workflow builder with 400+ integrations
- Self-hostable
- Perfect for agentic workflows

**Tailscale** (VPN):
- Zero-config WireGuard-based mesh VPN
- Secure remote access
- Works seamlessly on mobile

**Qdrant** (Vector Database):
- High-performance vector similarity search
- Ideal for RAG (Retrieval-Augmented Generation)
- Native gRPC and HTTP APIs
- Persistent storage for embeddings

**Flux CD** (GitOps):
- Declarative infrastructure management
- Automated Git ‚Üí Kubernetes deployments
- Multi-repository support
- Built-in drift detection and reconciliation

**Prometheus** (Observability):
- Prometheus: Metrics collection and storage

## Testing

Comprehensive health checking and verification tools available:

**Health Check Scripts**:
- `./infrastructure/compute-node/scripts/health-check.sh` - LLM services verification (compute node)
- `./scripts/health-check-all.sh` - Comprehensive service health checker (tests all services across both nodes)
- `./scripts/service-check-urls.sh` - URL-based service checker (tests actual deployed services)

**Quick Service Verification**:
```bash
# Check main services are responding
./scripts/service-check-urls.sh

# Full comprehensive check (including network and K8s status)
./scripts/health-check-all.sh
```

**Kubernetes Health**: Use standard `kubectl` commands for cluster status
```bash
kubectl get nodes                    # Node status
kubectl get pods -n homelab          # Service pod status
kubectl get svc -n homelab           # Service endpoints
```

**GitOps Health**: Use `flux check` and `flux get all` when deployed
```bash
kubectl get pods -n flux-system     # Flux controllers
kubectl get gitrepositories -n flux-system  # Git sync status
```

**Current Service Status** (as of latest check):
- ‚úÖ N8n: Healthy (port 30678)
- ‚úÖ Prometheus: Healthy (port 30090)
- ‚úÖ Loki: Healthy (port 30314)
- ‚úÖ Qdrant: Healthy (port 30633)
- ‚úÖ Docker Registry: Healthy (port 30500)
- ‚úÖ Homelab Dashboard: Healthy (port 30800)
- ‚úÖ LobeChat: Healthy (port 30910)
- ‚úÖ Mem0: Responding (port 30880)
- ‚úÖ Ollama: ‚ú® **NEW** - Healthy (port 30277, K8s deployment)
- ‚úÖ Whisper: ‚ú® **FIXED** - Healthy (port 30900, single replica)
- ‚ö†Ô∏è Family Assistant: Development in progress - dashboard API ready, frontend integration in progress
- ‚ùå Grafana: Removed (moved to trash)
- ‚ùå Open WebUI: Removed (StatefulSet deleted)
- ‚ùå Flowise: Removed (moved to trash)

## Deployment Notes

### Service Node Deployment (Completed)
All services deployed via `kubectl apply` to K3s cluster. No GitOps or Helm charts yet.

Current deployments in `homelab` namespace:
- PostgreSQL (postgres:16-alpine) - N8n and Family Assistant storage
- Redis (redis:7-alpine) - Cache and job queue
- N8n (n8nio/n8n) - Workflow automation
- Prometheus (prom/prometheus) - Metrics collection
- Qdrant (qdrant/qdrant) - Vector database for Mem0
- Loki (grafana/loki) - Log aggregation
- Homelab Dashboard (custom) - Unified landing page
- LobeChat (lobechat/lobechat) - AI chat interface with memory
- Mem0 (mem0/mem0) - AI memory layer
- Whisper (onerahmet/openai-whisper-asr-webservice) - Speech-to-text
- Ollama (ollama/ollama) - GPU-accelerated LLM inference
- Family Assistant (‚ö†Ô∏è Development) - Enhanced family platform with dashboard

### Compute Node Deployment (Completed)
Services deployed in Kubernetes `ollama` namespace:
1. ‚úÖ Ollama (ollama/ollama) deployed with GPU scheduling
2. ‚úÖ Accessible via NodePort service (port 30277)
3. ‚úÖ GPU acceleration configured via ROCm environment variables
4. ‚úÖ Persistent volume for model storage (10Gi)

**Note**: Native Ollama still available on compute node, K8s deployment now preferred

### GitOps Status

**Current Status**: ‚ö†Ô∏è Partially Functional

**‚úÖ Working Components**:
- Flux CD controllers deployed (6/6 pods healthy)
- Repository structure created in `clusters/homelab/`
- GitRepository configured (network connectivity issues identified)
- Infrastructure manifests organized and updated

**‚ö†Ô∏è Issues Identified**:
- GitRepository sync failing due to TLS handshake timeouts
- Network connectivity between cluster and GitHub blocked
- Infrastructure Kustomization conflicts with existing services
- Manual cleanup required for failed deployments

**üîß Resolution Options**:
1. **Network Fix**: Configure firewall/proxy rules for GitHub access
2. **Manual Bootstrap**: Use `flux bootstrap` with GitHub token
3. **Local Development**: Work with current deployed state

**Recent Changes Committed**:
- ‚úÖ Fixed Prometheus port conflicts (30190 vs 30090)
- ‚úÖ Updated Ollama K8s deployment with GPU scheduling
- ‚úÖ Removed deprecated services (Grafana, Flowise, Open WebUI)
- ‚úÖ Created comprehensive health check scripts

## Troubleshooting

### LLM Inference Issues
- Check Ollama service status via `systemctl`
- Verify GPU detection using `rocm-smi` and `rocminfo` (when ROCm installed)
- View Ollama logs via `journalctl`
- See `docs/LLM-SETUP.md` for detailed troubleshooting steps

### Service Node Issues
- SSH access: `ssh pesu@192.168.8.185`
- Check K3s status via `systemctl`
- View pod logs using `kubectl logs`
- Restart deployments using `kubectl rollout restart`

### Network Connectivity
- Verify Tailscale mesh status
- Test local network connectivity (192.168.8.0/24)
- Test inter-node communication (Tailscale IPs)

## Future Enhancements

## Performance Targets

### LLM Inference (Target)
- Latency: < 2s for first token
- Throughput: > 20 tokens/second on 7B models
- GPU Utilization: 60-80%
- Model Loading: < 10s

### System Performance (Target)
- API Response: < 200ms (P95)
- Workflow Execution: < 5 seconds (P95)
- CPU Usage: < 70% average
- Memory Usage: < 75% average

### Current Performance (Achieved)
- GPU-accelerated inference: 20-30 tokens/second on 7B models (ROCm 6.4.1 + RX 7800 XT)
- VRAM usage: ~6-7GB for Q4 quantized 7B models, ~9GB for 14B models
- CPU fallback if needed: ~2-5 tokens/second
- Models loaded: qwen2.5-coder:14b (9.0GB), llama3.1:8b (4.9GB), mistral:7b (4.4GB), nomic-embed-text (274MB)
- Current state: ‚úÖ LLM services fully operational with GPU acceleration

## Session Continuity

When resuming work, check:
1. `docs/SESSION-STATE.md` for current task and progress (updated daily)
2. `README.md` roadmap section for sprint status
3. Service health: N8n, Prometheus, Homelab Dashboard accessible
4. LLM health: `./infrastructure/compute-node/scripts/health-check.sh`
5. MCP tools: All 5 development tools ready after Claude Code restart
6. Family Assistant: Check deployment status and dashboard progress

## Important Notes

- **IP Addressing**: Documentation sometimes references 192.168.8.0/24, but actual network is 192.168.8.0/24
- **Storage**: Service node has limited disk (98GB) - monitor usage
- **No root access needed**: Both nodes have passwordless sudo configured for user `pesu`

## Git Workflow

- Main development branch: `main`
- Commit messages should be descriptive and reference sprint/task when relevant
- All infrastructure and configuration is version controlled
- Secrets go in `.env` (gitignored), not in code

---

## üìç Where We Left Off

**Last Session**: 2025-11-15 - Creating Claude Code End-of-Session Hook
**Working Directory**: `/home/pesu/Rakuflow/systems/homelab`
**Current Branch**: `main`

### Active Tasks
- **In Progress**: Creating end-of-session hook for documentation updates and cleanup
- **Completed**: Documentation structure added to CLAUDE.md
- **Next**: Implement the actual hook script with cleanup and session management

### Tools & Services Being Used
- Claude Code with custom hooks
- Project documentation system
- File cleanup automation

### Continuation Instructions
1. Complete implementation of end-of-session hook script
2. Test documentation updates and cleanup functionality
3. Verify "Where We Left Off" section updates correctly
4. Commit and push the hook implementation

### Quick Status Check
- [ ] Check `docs/SESSION-STATE.md` for detailed session progress
- [ ] Verify git status for any uncommitted changes
- [ ] Review current sprint objectives in README.md

---

## Support Resources

- **Project Issues**: https://github.com/pesuga/homelab/issues
- **Ollama Docs**: https://github.com/ollama/ollama
- **LiteLLM Docs**: https://docs.litellm.ai
- **N8n Docs**: https://docs.n8n.io
- **K3s Docs**: https://docs.k3s.io
- **ROCm Docs**: https://rocm.docs.amd.com
